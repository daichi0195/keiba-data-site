#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹å–„ç‰ˆã‚¿ã‚¤ãƒ æŒ‡æ•°ï¼ˆ*_improvedï¼‰ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

æ”¹å–„å†…å®¹ï¼š
1. Fallbackéšå±¤ã®é †ç•ªå¤‰æ›´ï¼ˆ2ã¨3ã‚’å…¥ã‚Œæ›¿ãˆï¼‰
2. fallback_level >= 3ã®å ´åˆã€ä¿¡é ¼åº¦ã‚’0.7å€ã«èª¿æ•´
"""
from google.cloud import bigquery
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
import pickle

PROJECT_ID = "umadata"
DATASET_ID = "keiba_data"

def load_data_from_bigquery():
    """BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    client = bigquery.Client(project=PROJECT_ID)

    print("=" * 100)
    print("ğŸ“Š BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆæ”¹å–„ç‰ˆï¼‰...")
    print("=" * 100)

    query = """
    SELECT
        race_id,
        race_date,
        racecourse,
        surface,
        distance,
        going,
        race_class,
        horse_id,
        horse_name,
        finish_position,
        popularity,
        odds,

        -- åŸºæœ¬æƒ…å ±
        horse_number,
        bracket_number,
        sex,
        age,
        horse_weight,
        weight_change,
        days_since_last_race,

        -- é¨æ‰‹æƒ…å ±
        jockey_id,
        is_jockey_change,
        jockey_place_rate_surface_distance,
        jockey_rides_surface_distance,

        -- è„šè³ªæƒ…å ±
        running_style_last1,
        running_style_mode,
        running_style_mode_win_rate,
        running_style_last1_win_rate,

        -- éå»æˆç¸¾
        finish_pos_best_last5,

        -- æ”¹å–„ç‰ˆã‚¿ã‚¤ãƒ æŒ‡æ•°ï¼ˆpast 1-3ï¼‰
        time_index_zscore_last1_improved,
        time_index_zscore_last2_improved,
        time_index_zscore_last3_improved,

        -- æ”¹å–„ç‰ˆã‚¿ã‚¤ãƒ æŒ‡æ•°é›†ç´„
        time_index_zscore_mean_3_improved,
        time_index_zscore_best_3_improved,
        time_index_zscore_worst_3_improved,
        time_index_zscore_trend_3_improved,

        -- æ”¹å–„ç‰ˆãƒ©ã‚¹ãƒˆ3FæŒ‡æ•°
        last3f_index_zscore_last1_improved,
        last3f_index_zscore_last2_improved,

        -- ä¼‘é¤Šé–¢é€£
        is_after_long_rest,
        is_consecutive_race,
        is_debut,
        rest_period_category,

        -- Fallback levelï¼ˆåˆ†æç”¨ï¼‰
        fallback_level_last1,
        fallback_level_last2,
        fallback_level_last3

    FROM `umadata.keiba_data.all_features_complete_improved`
    WHERE race_date >= '2021-01-01'
        AND finish_position IS NOT NULL
    ORDER BY race_date
    """

    df = client.query(query).to_dataframe()

    # race_dateã‚’æ—¥ä»˜å‹ã«å¤‰æ›
    df['race_date'] = pd.to_datetime(df['race_date'])

    print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"   ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}è¡Œ")
    print(f"   æœŸé–“: {df['race_date'].min()} ~ {df['race_date'].max()}")
    print(f"   ç·ãƒ¬ãƒ¼ã‚¹æ•°: {df['race_id'].nunique():,}ãƒ¬ãƒ¼ã‚¹")

    # Fallback levelåˆ†å¸ƒ
    print(f"\nã€Fallback Levelåˆ†å¸ƒã€‘")
    for level in [1, 2, 3, 5]:
        count = (df['fallback_level_last1'] == level).sum()
        if count > 0:
            pct = count / len(df) * 100
            print(f"   Level {level}: {count:,}ä»¶ ({pct:.2f}%)")

    return df

def prepare_features(df):
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    print("\n" + "=" * 100)
    print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
    print("=" * 100)

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    racecourse_map = {'æœ­å¹Œ': 1, 'å‡½é¤¨': 2, 'ç¦å³¶': 3, 'æ–°æ½Ÿ': 4, 'æ±äº¬': 5, 'ä¸­å±±': 6, 'ä¸­äº¬': 7, 'äº¬éƒ½': 8, 'é˜ªç¥': 9, 'å°å€‰': 10}
    surface_map = {'èŠ': 0, 'ãƒ€ãƒ¼ãƒˆ': 1}
    going_map = {'è‰¯': 0, 'ã‚„ã‚„é‡': 1, 'é‡': 2, 'ä¸è‰¯': 3}
    race_class_map = {'æ–°é¦¬': 0, 'æœªå‹åˆ©': 1, 'ï¼‘å‹ã‚¯ãƒ©ã‚¹': 2, 'ï¼’å‹ã‚¯ãƒ©ã‚¹': 3, 'ï¼“å‹ã‚¯ãƒ©ã‚¹': 4, 'ã‚ªãƒ¼ãƒ—ãƒ³': 5}

    df['racecourse_encoded'] = df['racecourse'].map(racecourse_map).fillna(0)
    df['surface_encoded'] = df['surface'].map(surface_map).fillna(0)
    df['going_encoded'] = df['going'].map(going_map).fillna(0)
    df['race_class_encoded'] = df['race_class'].map(race_class_map).fillna(5)

    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆ33å€‹ï¼‰
    features = [
        # è„šè³ªï¼ˆ4å€‹ï¼‰
        'running_style_last1', 'running_style_mode',
        'running_style_mode_win_rate', 'running_style_last1_win_rate',

        # é¨æ‰‹ï¼ˆ3å€‹ï¼‰
        'jockey_rides_surface_distance',
        'jockey_place_rate_surface_distance',
        'is_jockey_change',

        # é¦¬ã®æˆç¸¾ï¼ˆ1å€‹ï¼‰
        'finish_pos_best_last5',

        # ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ï¼ˆ4å€‹ï¼‰
        'racecourse_encoded', 'surface_encoded', 'going_encoded', 'race_class_encoded',

        # é¦¬ã®åŸºæœ¬æƒ…å ±ï¼ˆ8å€‹ï¼‰
        'distance', 'sex', 'age', 'horse_weight', 'weight_change',
        'bracket_number', 'horse_number', 'days_since_last_race',

        # æ”¹å–„ç‰ˆã‚¿ã‚¤ãƒ æŒ‡æ•°ï¼ˆpast 1-3ã€3å€‹ï¼‰
        'time_index_zscore_last1_improved',
        'time_index_zscore_last2_improved',
        'time_index_zscore_last3_improved',

        # æ”¹å–„ç‰ˆã‚¿ã‚¤ãƒ æŒ‡æ•°é›†ç´„ï¼ˆ4å€‹ï¼‰
        'time_index_zscore_mean_3_improved',
        'time_index_zscore_best_3_improved',
        'time_index_zscore_worst_3_improved',
        'time_index_zscore_trend_3_improved',

        # æ”¹å–„ç‰ˆãƒ©ã‚¹ãƒˆ3FæŒ‡æ•°ï¼ˆ2å€‹ï¼‰
        'last3f_index_zscore_last1_improved',
        'last3f_index_zscore_last2_improved',

        # ä¼‘é¤Šé–¢é€£ï¼ˆ4å€‹ï¼‰
        'is_after_long_rest', 'is_consecutive_race', 'is_debut', 'rest_period_category'
    ]

    print(f"\nâœ… ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {len(features)}å€‹")
    print(f"\nã€ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã€‘")
    for i, feat in enumerate(features, 1):
        print(f"  {i:2d}. {feat}")

    # æ¬ æå€¤å‡¦ç†
    print(f"\nã€æ¬ æå€¤ã®çŠ¶æ³ã€‘")
    for feat in features:
        null_count = df[feat].isnull().sum()
        null_pct = null_count / len(df) * 100
        if null_pct > 0:
            print(f"  {feat}: {null_count:,}å€‹ ({null_pct:.1f}%)")

    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    X = df[features].fillna(0)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆ1ç€=1, ãã‚Œä»¥å¤–=0ï¼‰
    y = (df['finish_position'] == 1).astype(int)

    print(f"\nâœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†")
    print(f"   1ç€ç‡: {y.mean()*100:.2f}%")

    return X, y, features

def train_and_evaluate(df, features_list):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦è©•ä¾¡"""
    print("\n" + "=" * 100)
    print("ğŸš‚ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæ”¹å–„ç‰ˆï¼‰")
    print("=" * 100)

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆ2024-11-01ã§train/testï¼‰
    train_df = df[df['race_date'] < '2024-11-01'].copy()
    test_df = df[df['race_date'] >= '2024-11-01'].copy()

    print(f"\nã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã€‘")
    print(f"  è¨“ç·´æœŸé–“: {train_df['race_date'].min()} ~ {train_df['race_date'].max()}")
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,}è¡Œï¼ˆ{train_df['race_id'].nunique():,}ãƒ¬ãƒ¼ã‚¹ï¼‰")
    print(f"  ãƒ†ã‚¹ãƒˆæœŸé–“: {test_df['race_date'].min()} ~ {test_df['race_date'].max()}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}è¡Œï¼ˆ{test_df['race_id'].nunique():,}ãƒ¬ãƒ¼ã‚¹ï¼‰")

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
    X_train, y_train, features = prepare_features(train_df)
    X_test, y_test, _ = prepare_features(test_df)

    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'seed': 42
    }

    print(f"\nã€LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
    for key, value in params.items():
        print(f"  {key}: {value}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

    # è¨“ç·´
    print(f"\nğŸƒ è¨“ç·´é–‹å§‹...")
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[train_data, test_data],
        valid_names=['train', 'test'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )

    print(f"\nâœ… è¨“ç·´å®Œäº†ï¼ˆbest iteration: {model.best_iteration}ï¼‰")

    # äºˆæ¸¬
    y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)
    y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)

    # AUCè©•ä¾¡
    auc_train = roc_auc_score(y_train, y_pred_train)
    auc_test = roc_auc_score(y_test, y_pred_test)

    print("\n" + "=" * 100)
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆæ”¹å–„ç‰ˆï¼‰")
    print("=" * 100)
    print(f"\nã€AUCã€‘")
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {auc_train:.4f}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {auc_test:.4f}")

    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)

    print(f"\nã€ç‰¹å¾´é‡é‡è¦åº¦ Top20ã€‘")
    print(feature_importance.head(20).to_string(index=False))

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬çµæœã‚’ä¿å­˜
    test_df['predicted_prob'] = y_pred_test
    test_df['predicted_rank'] = test_df.groupby('race_id')['predicted_prob'].rank(ascending=False, method='first')

    # é©ä¸­ç‡ãƒ»å›åç‡è©•ä¾¡
    evaluate_betting_performance(test_df)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save_model('model_improved_time_index.txt')
    with open('model_improved_time_index.pkl', 'wb') as f:
        pickle.dump(model, f)

    print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")
    print(f"   - model_improved_time_index.txt")
    print(f"   - model_improved_time_index.pkl")

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’CSVä¿å­˜
    feature_importance.to_csv('data/feature_importance_improved.csv', index=False, encoding='utf-8-sig')
    print(f"   - data/feature_importance_improved.csv")

    return model, test_df, feature_importance

def evaluate_betting_performance(test_df):
    """çš„ä¸­ç‡ãƒ»å›åç‡ã‚’è©•ä¾¡"""
    print("\n" + "=" * 100)
    print("ğŸ¯ é¦¬åˆ¸ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäºˆæ¸¬1ä½ã®é¦¬ã‚’å˜å‹è³¼å…¥ï¼‰")
    print("=" * 100)

    # äºˆæ¸¬1ä½ã®é¦¬ã®ã¿æŠ½å‡º
    predicted_winners = test_df[test_df['predicted_rank'] == 1].copy()

    total_races = len(predicted_winners)
    wins = (predicted_winners['finish_position'] == 1).sum()
    win_rate = wins / total_races * 100

    # å›åç‡è¨ˆç®—ï¼ˆ100å††ãƒ™ãƒƒãƒˆï¼‰
    bet_amount = 100
    total_bet = total_races * bet_amount

    # çš„ä¸­æ™‚ã®æ‰•ã„æˆ»ã—
    winning_bets = predicted_winners[predicted_winners['finish_position'] == 1]
    total_return = (winning_bets['odds'] * bet_amount).sum()

    recovery_rate = (total_return / total_bet) * 100

    print(f"\nã€çµæœã€‘")
    print(f"  ç·ãƒ¬ãƒ¼ã‚¹æ•°: {total_races:,}ãƒ¬ãƒ¼ã‚¹")
    print(f"  çš„ä¸­æ•°: {wins}ãƒ¬ãƒ¼ã‚¹")
    print(f"  çš„ä¸­ç‡: {win_rate:.2f}%")
    print(f"  ç·æŠ•è³‡é¡: {total_bet:,}å††")
    print(f"  ç·æ‰•æˆ»é¡: {total_return:,.0f}å††")
    print(f"  å›åç‡: {recovery_rate:.2f}%")
    print(f"  æç›Š: {total_return - total_bet:,.0f}å††")

    # äººæ°—åˆ¥ã®çš„ä¸­ç‡
    print(f"\nã€äºˆæ¸¬1ä½é¦¬ã®äººæ°—åˆ†å¸ƒã€‘")
    popularity_dist = predicted_winners.groupby('popularity').agg({
        'race_id': 'count',
        'finish_position': lambda x: (x == 1).sum()
    }).rename(columns={'race_id': 'count', 'finish_position': 'wins'})
    popularity_dist['win_rate'] = popularity_dist['wins'] / popularity_dist['count'] * 100

    for pop in sorted(popularity_dist.index)[:10]:
        row = popularity_dist.loc[pop]
        print(f"  {pop}ç•ªäººæ°—: {row['wins']}/{row['count']}å‹ ({row['win_rate']:.1f}%)")

def main():
    print("\n" + "=" * 100)
    print("ğŸ æ”¹å–„ç‰ˆã‚¿ã‚¤ãƒ æŒ‡æ•°ã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«è¨“ç·´")
    print("=" * 100)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_data_from_bigquery()

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»è©•ä¾¡
    model, test_df, feature_importance = train_and_evaluate(df, None)

    print("\n" + "=" * 100)
    print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print("=" * 100)

if __name__ == '__main__':
    main()
