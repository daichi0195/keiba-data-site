#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‘ã‚¿ãƒ¼ãƒ³C v3: é¨æ‰‹ï¼ˆå‹ç‡+è¤‡å‹ç‡ï¼‰+ èª¿æ•™å¸«ï¼ˆå‹ç‡+è¤‡å‹ç‡ï¼‰ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆã€‘

å¤‰æ›´ç‚¹:
- v2ã®ç‰¹å¾´é‡ï¼ˆ28å€‹ï¼‰ã«åŠ ãˆã¦:
  1. trainer_win_rate_surface_distanceï¼ˆèª¿æ•™å¸«å‹ç‡ï¼‰
  2. trainer_place_rate_surface_distanceï¼ˆèª¿æ•™å¸«è¤‡å‹ç‡ï¼‰
  3. finish_position_last1ï¼ˆå‰èµ°ã®ç€é †ï¼‰
  4. finish_pos_avg_last5ï¼ˆç›´è¿‘5èµ°ã®å¹³å‡ç€é †ï¼‰

ç‰¹å¾´é‡æ•°: 28å€‹ â†’ 31å€‹

æ³¨æ„:
- running_styleå‹ç‡çµ±è¨ˆã¯å‰Šé™¤ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã®ãŸã‚ï¼‰
- finish_pos_best_last5ã‚’ finish_position_last1 + finish_pos_avg_last5 ã«åˆ†å‰²
- all_features_complete_no_leakage ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
"""
from google.cloud import bigquery
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
import pickle
import os

PROJECT_ID = "umadata"
DATASET_ID = "keiba_data"

def load_data_from_bigquery():
    """BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    client = bigquery.Client(project=PROJECT_ID)

    print("=" * 100)
    print("ğŸ“Š BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³C v3: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆï¼‰...")
    print("=" * 100)

    query = """
    SELECT
        race_id,
        current_race_date as race_date,
        racecourse,
        surface,
        distance,
        race_class,
        horse_id,
        horse_name,
        finish_position,
        popularity,
        odds,

        -- åŸºæœ¬æƒ…å ±
        horse_number,
        bracket_number,
        sex,
        age,
        horse_weight,
        weight_change,
        days_since_last_race,

        -- é¨æ‰‹æƒ…å ±ï¼ˆè¤‡å‹ç‡ + å‹ç‡ã®ä¸¡æ–¹ã‚’ä½¿ç”¨ï¼‰
        jockey_id,
        is_jockey_change,
        jockey_place_rate_surface_distance,
        jockey_win_rate_surface_distance,
        detailed_rides as jockey_rides_surface_distance,

        -- èª¿æ•™å¸«æƒ…å ±
        trainer_id,
        trainer_place_rate_surface_distance,
        trainer_win_rate_surface_distance,

        -- è„šè³ªæƒ…å ±
        running_style_last1,
        running_style_mode,

        -- éå»æˆç¸¾ï¼ˆç€é †ï¼‰
        finish_position_last1,
        finish_position_last2,
        finish_position_last3,
        finish_position_last4,
        finish_position_last5,

        -- ã‚¿ã‚¤ãƒ æŒ‡æ•°ï¼ˆpast 1-3ï¼‰
        time_index_zscore_last1,
        time_index_zscore_last2,
        time_index_zscore_last3,

        -- ã‚¿ã‚¤ãƒ æŒ‡æ•°é›†ç´„ï¼ˆæ”¹å–„ç‰ˆï¼‰
        time_index_zscore_mean_3_improved,
        time_index_zscore_best_3_improved,
        time_index_zscore_worst_3_improved,
        time_index_zscore_trend_3_improved,

        -- ãƒ©ã‚¹ãƒˆ3FæŒ‡æ•°ï¼ˆæ”¹å–„ç‰ˆï¼‰
        last3f_index_zscore_last1_improved,
        last3f_index_zscore_last2_improved,

        -- ä¼‘é¤Šé–¢é€£
        rest_period_category

    FROM `umadata.keiba_data.all_features_complete_no_leakage`
    WHERE current_race_date >= '2021-01-01'
        AND finish_position IS NOT NULL
    ORDER BY current_race_date
    """

    df = client.query(query).to_dataframe()

    # race_dateã‚’æ—¥ä»˜å‹ã«å¤‰æ›
    df['race_date'] = pd.to_datetime(df['race_date'])

    print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"   ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}è¡Œ")
    print(f"   æœŸé–“: {df['race_date'].min()} ~ {df['race_date'].max()}")
    print(f"   ç·ãƒ¬ãƒ¼ã‚¹æ•°: {df['race_id'].nunique():,}ãƒ¬ãƒ¼ã‚¹")

    return df

def prepare_features(df):
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    print("\n" + "=" * 100)
    print("ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³C v3: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆï¼‰")
    print("=" * 100)

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    racecourse_map = {'æœ­å¹Œ': 1, 'å‡½é¤¨': 2, 'ç¦å³¶': 3, 'æ–°æ½Ÿ': 4, 'æ±äº¬': 5, 'ä¸­å±±': 6, 'ä¸­äº¬': 7, 'äº¬éƒ½': 8, 'é˜ªç¥': 9, 'å°å€‰': 10}
    surface_map = {'èŠ': 0, 'ãƒ€ãƒ¼ãƒˆ': 1}
    race_class_map = {'æ–°é¦¬': 0, 'æœªå‹åˆ©': 1, 'ï¼‘å‹ã‚¯ãƒ©ã‚¹': 2, 'ï¼’å‹ã‚¯ãƒ©ã‚¹': 3, 'ï¼“å‹ã‚¯ãƒ©ã‚¹': 4, 'ã‚ªãƒ¼ãƒ—ãƒ³': 5}

    df['racecourse_encoded'] = df['racecourse'].map(racecourse_map).fillna(0)
    df['surface_encoded'] = df['surface'].map(surface_map).fillna(0)
    df['race_class_encoded'] = df['race_class'].map(race_class_map).fillna(5)

    # ç›´è¿‘5èµ°ã®å¹³å‡ç€é †ã‚’è¨ˆç®—
    df['finish_pos_avg_last5'] = df[['finish_position_last1', 'finish_position_last2',
                                       'finish_position_last3', 'finish_position_last4',
                                       'finish_position_last5']].mean(axis=1)

    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆ31å€‹ï¼‰
    features = [
        # è„šè³ªï¼ˆ2å€‹ï¼‰- å‹ç‡çµ±è¨ˆã¯å‰Šé™¤
        'running_style_last1', 'running_style_mode',

        # é¨æ‰‹ï¼ˆ4å€‹ï¼‰
        'jockey_rides_surface_distance',
        'jockey_place_rate_surface_distance',
        'jockey_win_rate_surface_distance',
        'is_jockey_change',

        # èª¿æ•™å¸«ï¼ˆ2å€‹ï¼‰
        'trainer_place_rate_surface_distance',
        'trainer_win_rate_surface_distance',

        # é¦¬ã®æˆç¸¾ï¼ˆ2å€‹ï¼‰- ãƒ™ã‚¹ãƒˆç€é †ã‹ã‚‰å‰èµ°ç€é †+å¹³å‡ç€é †ã«å¤‰æ›´
        'finish_position_last1',
        'finish_pos_avg_last5',

        # ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ï¼ˆ3å€‹ï¼‰
        'racecourse_encoded', 'surface_encoded', 'race_class_encoded',

        # é¦¬ã®åŸºæœ¬æƒ…å ±ï¼ˆ8å€‹ï¼‰
        'distance', 'sex', 'age', 'horse_weight', 'weight_change',
        'bracket_number', 'horse_number', 'days_since_last_race',

        # ã‚¿ã‚¤ãƒ æŒ‡æ•°ï¼ˆpast 1-3ã€3å€‹ï¼‰
        'time_index_zscore_last1',
        'time_index_zscore_last2',
        'time_index_zscore_last3',

        # ã‚¿ã‚¤ãƒ æŒ‡æ•°é›†ç´„ï¼ˆæ”¹å–„ç‰ˆã€4å€‹ï¼‰
        'time_index_zscore_mean_3_improved',
        'time_index_zscore_best_3_improved',
        'time_index_zscore_worst_3_improved',
        'time_index_zscore_trend_3_improved',

        # ãƒ©ã‚¹ãƒˆ3FæŒ‡æ•°ï¼ˆæ”¹å–„ç‰ˆã€2å€‹ï¼‰
        'last3f_index_zscore_last1_improved',
        'last3f_index_zscore_last2_improved',

        # ä¼‘é¤Šé–¢é€£ï¼ˆ1å€‹ï¼‰
        'rest_period_category'
    ]

    print(f"\nâœ… ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {len(features)}å€‹")
    print(f"\nã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸å¯¾ç­–ã€‘")
    print(f"  - all_features_complete_no_leakage ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨")
    print(f"  - é¨æ‰‹ãƒ»èª¿æ•™å¸«çµ±è¨ˆã¯å„ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ã‚ˆã‚Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§è¨ˆç®—")
    print(f"  - è„šè³ªå‹ç‡çµ±è¨ˆã¯å‰Šé™¤ï¼ˆãƒªãƒ¼ã‚±ãƒ¼ã‚¸ãƒªã‚¹ã‚¯ã®ãŸã‚ï¼‰")

    # æ¬ æå€¤å‡¦ç†
    print(f"\nã€æ¬ æå€¤ã®çŠ¶æ³ã€‘")
    for feat in features:
        null_count = df[feat].isnull().sum()
        null_pct = null_count / len(df) * 100
        if null_pct > 0:
            print(f"  {feat}: {null_count:,}å€‹ ({null_pct:.1f}%)")

    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    X = df[features].fillna(0)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆ1ç€=1, ãã‚Œä»¥å¤–=0ï¼‰
    y = (df['finish_position'] == 1).astype(int)

    print(f"\nâœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†")
    print(f"   1ç€ç‡: {y.mean()*100:.2f}%")

    return X, y, features

def train_and_evaluate(df):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦è©•ä¾¡"""
    print("\n" + "=" * 100)
    print("ğŸš‚ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³C v3: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆï¼‰")
    print("=" * 100)

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆ2024-11-01ã§train/testï¼‰
    train_df = df[df['race_date'] < '2024-11-01'].copy()
    test_df = df[df['race_date'] >= '2024-11-01'].copy()

    print(f"\nã€ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã€‘")
    print(f"  è¨“ç·´æœŸé–“: {train_df['race_date'].min()} ~ {train_df['race_date'].max()}")
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,}è¡Œï¼ˆ{train_df['race_id'].nunique():,}ãƒ¬ãƒ¼ã‚¹ï¼‰")
    print(f"  ãƒ†ã‚¹ãƒˆæœŸé–“: {test_df['race_date'].min()} ~ {test_df['race_date'].max()}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df):,}è¡Œï¼ˆ{test_df['race_id'].nunique():,}ãƒ¬ãƒ¼ã‚¹ï¼‰")

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
    X_train, y_train, features = prepare_features(train_df)
    X_test, y_test, _ = prepare_features(test_df)

    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'seed': 42
    }

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

    # è¨“ç·´
    print(f"\nğŸƒ è¨“ç·´é–‹å§‹...")
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[train_data, test_data],
        valid_names=['train', 'test'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )

    print(f"\nâœ… è¨“ç·´å®Œäº†ï¼ˆbest iteration: {model.best_iteration}ï¼‰")

    # äºˆæ¸¬
    y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)
    y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)

    # AUCè©•ä¾¡
    auc_train = roc_auc_score(y_train, y_pred_train)
    auc_test = roc_auc_score(y_test, y_pred_test)

    print("\n" + "=" * 100)
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³C v3: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆï¼‰")
    print("=" * 100)
    print(f"\nã€AUCã€‘")
    print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {auc_train:.4f}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {auc_test:.4f}")

    # ç‰¹å¾´é‡é‡è¦åº¦
    feature_importance = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False)

    print(f"\nã€ç‰¹å¾´é‡é‡è¦åº¦ Top20ã€‘")
    print(feature_importance.head(20).to_string(index=False))

    # èª¿æ•™å¸«çµ±è¨ˆã®é‡è¦åº¦ã‚’ç¢ºèª
    trainer_features = feature_importance[feature_importance['feature'].str.contains('trainer')]
    if len(trainer_features) > 0:
        print(f"\nã€èª¿æ•™å¸«çµ±è¨ˆã®é‡è¦åº¦ã€‘")
        for idx, row in trainer_features.iterrows():
            rank = feature_importance[feature_importance['feature'] == row['feature']].index[0] + 1
            pct = row['importance'] / feature_importance['importance'].sum() * 100
            print(f"  {rank:2d}ä½: {row['feature']:45s} {row['importance']:>10.0f} ({pct:5.2f}%)")

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬çµæœã‚’ä¿å­˜
    test_df['predicted_prob'] = y_pred_test
    test_df['predicted_rank'] = test_df.groupby('race_id')['predicted_prob'].rank(ascending=False, method='first')

    # é©ä¸­ç‡ãƒ»å›åç‡è©•ä¾¡
    evaluate_betting_performance(test_df)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs('../../models', exist_ok=True)
    os.makedirs('../../data/evaluations', exist_ok=True)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save_model('../../models/model_pattern_c_v3_no_leakage.txt')
    with open('../../models/model_pattern_c_v3_no_leakage.pkl', 'wb') as f:
        pickle.dump(model, f)

    print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")
    print(f"   - models/model_pattern_c_v3_no_leakage.txt")
    print(f"   - models/model_pattern_c_v3_no_leakage.pkl")

    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’CSVä¿å­˜
    feature_importance.to_csv('../../data/evaluations/feature_importance_pattern_c_v3_no_leakage.csv', index=False, encoding='utf-8-sig')
    print(f"   - data/evaluations/feature_importance_pattern_c_v3_no_leakage.csv")

    return model, test_df, feature_importance, auc_test

def evaluate_betting_performance(test_df):
    """çš„ä¸­ç‡ãƒ»å›åç‡ã‚’è©•ä¾¡"""
    print("\n" + "=" * 100)
    print("ğŸ¯ é¦¬åˆ¸ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäºˆæ¸¬1ä½ã®é¦¬ã‚’å˜å‹è³¼å…¥ï¼‰")
    print("=" * 100)

    # äºˆæ¸¬1ä½ã®é¦¬ã®ã¿æŠ½å‡º
    predicted_winners = test_df[test_df['predicted_rank'] == 1].copy()

    total_races = len(predicted_winners)
    wins = (predicted_winners['finish_position'] == 1).sum()
    win_rate = wins / total_races * 100

    # å›åç‡è¨ˆç®—ï¼ˆ100å††ãƒ™ãƒƒãƒˆï¼‰
    bet_amount = 100
    total_bet = total_races * bet_amount

    # çš„ä¸­æ™‚ã®æ‰•ã„æˆ»ã—
    winning_bets = predicted_winners[predicted_winners['finish_position'] == 1]
    total_return = (winning_bets['odds'] * bet_amount).sum()

    recovery_rate = (total_return / total_bet) * 100

    print(f"\nã€å…¨ãƒ¬ãƒ¼ã‚¹è³¼å…¥ã€‘")
    print(f"  ç·ãƒ¬ãƒ¼ã‚¹æ•°: {total_races:,}ãƒ¬ãƒ¼ã‚¹")
    print(f"  çš„ä¸­æ•°: {wins}ãƒ¬ãƒ¼ã‚¹")
    print(f"  çš„ä¸­ç‡: {win_rate:.2f}%")
    print(f"  ç·æŠ•è³‡é¡: {total_bet:,}å††")
    print(f"  ç·æ‰•æˆ»é¡: {total_return:,.0f}å††")
    print(f"  å›åç‡: {recovery_rate:.2f}%")
    print(f"  æç›Š: {total_return - total_bet:,.0f}å††")

    # è¤‡æ•°ã®é–¾å€¤ã§è©•ä¾¡
    print(f"\nã€é–¾å€¤åˆ¥æˆ¦ç•¥åˆ†æã€‘")
    print(f"{'='*100}")
    print(f"{'é–¾å€¤':>6} {'ãƒ™ãƒƒãƒˆæ•°':>10} {'çš„ä¸­æ•°':>10} {'çš„ä¸­ç‡':>10} {'æŠ•è³‡é¡':>12} {'æ‰•æˆ»é¡':>12} {'å›åç‡':>10} {'æç›Š':>12}")
    print(f"{'-'*100}")

    thresholds = [0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80]

    results = []
    for threshold in thresholds:
        filtered = predicted_winners[predicted_winners['predicted_prob'] >= threshold]
        if len(filtered) > 0:
            bet_count = len(filtered)
            hit_count = (filtered['finish_position'] == 1).sum()
            hit_rate = hit_count / bet_count * 100
            total_bet = bet_count * bet_amount
            winning = filtered[filtered['finish_position'] == 1]
            total_return = (winning['odds'] * bet_amount).sum()
            recovery = (total_return / total_bet) * 100
            profit = total_return - total_bet

            # å›åç‡100%ä»¥ä¸Šã®å ´åˆã¯å¼·èª¿
            marker = " âœ…" if recovery >= 100 else ""

            print(f"{threshold:>6.2f} {bet_count:>10,} {hit_count:>10,} {hit_rate:>9.2f}% {total_bet:>11,}å†† {total_return:>11,.0f}å†† {recovery:>9.2f}% {profit:>11,.0f}å††{marker}")

            results.append({
                'threshold': threshold,
                'bet_count': bet_count,
                'hit_rate': hit_rate,
                'recovery_rate': recovery,
                'profit': profit
            })
        else:
            print(f"{threshold:>6.2f} {'è©²å½“ãªã—':>10}")

    # æœ€ã‚‚å›åç‡ãŒé«˜ã„é–¾å€¤ã‚’è¡¨ç¤º
    if results:
        best_recovery = max(results, key=lambda x: x['recovery_rate'])
        best_profit = max(results, key=lambda x: x['profit'])

        print(f"\n{'='*100}")
        print(f"ã€æœ€é©é–¾å€¤åˆ†æã€‘")
        print(f"  æœ€é«˜å›åç‡: é–¾å€¤{best_recovery['threshold']:.2f} â†’ å›åç‡{best_recovery['recovery_rate']:.2f}% ï¼ˆ{best_recovery['bet_count']}ãƒ¬ãƒ¼ã‚¹ï¼‰")
        print(f"  æœ€å¤§åˆ©ç›Š:   é–¾å€¤{best_profit['threshold']:.2f} â†’ æç›Š{best_profit['profit']:+,.0f}å†† ï¼ˆ{best_profit['bet_count']}ãƒ¬ãƒ¼ã‚¹ï¼‰")

        # å›åç‡100%ä»¥ä¸Šã®é–¾å€¤ã‚’æŠ½å‡º
        profitable = [r for r in results if r['recovery_rate'] >= 100]
        if profitable:
            print(f"\n  ã€ãƒ—ãƒ©ã‚¹åæ”¯ã®é–¾å€¤ã€‘ï¼ˆ{len(profitable)}å€‹ï¼‰")
            for r in profitable:
                print(f"    é–¾å€¤{r['threshold']:.2f}: å›åç‡{r['recovery_rate']:.2f}% / {r['bet_count']}ãƒ¬ãƒ¼ã‚¹ / æç›Š{r['profit']:+,.0f}å††")
        else:
            print(f"\n  âš ï¸ å›åç‡100%ä»¥ä¸Šã®é–¾å€¤ã¯ã‚ã‚Šã¾ã›ã‚“")

def main():
    print("\n" + "=" * 100)
    print("ğŸ ãƒ‘ã‚¿ãƒ¼ãƒ³C v3: é¨æ‰‹çµ±è¨ˆ + èª¿æ•™å¸«çµ±è¨ˆã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆã€‘")
    print("=" * 100)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_data_from_bigquery()

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»è©•ä¾¡
    model, test_df, feature_importance, auc_test = train_and_evaluate(df)

    print("\n" + "=" * 100)
    print("âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³C v3è¨“ç·´å®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ä¿®æ­£ç‰ˆï¼‰")
    print(f"   ç‰¹å¾´é‡æ•°: 31å€‹")
    print(f"   è¿½åŠ : trainerçµ±è¨ˆã€é¦¬ã®æˆç¸¾ã‚’å‰èµ°ç€é †+å¹³å‡ç€é †ã«åˆ†å‰²")
    print(f"   å‰Šé™¤: running_styleå‹ç‡çµ±è¨ˆï¼ˆãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼‰")
    print(f"   AUC: {auc_test:.4f}")
    print("=" * 100)

if __name__ == '__main__':
    main()
